# Tokenizers

The tokenizers defined in this folder are unlike traditional tokenizers commonly seen in NLP tasks as they map high-dimensional inputs like images to tokens for a decision transformer. As a result these tokenizers are in and of themselves models that need to be learnt.

The above is in contrast with tokenizers which traditionally process input text into tokens that serve as suitable inputs to a model.
