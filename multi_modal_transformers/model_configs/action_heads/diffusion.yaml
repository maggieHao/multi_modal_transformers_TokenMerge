diffusion_action_head:
  _target_: multi_modal_transformers.action_heads.diffusion.DiffusionActionHead
  attention_pooling:
    _target_: multi_modal_transformers.attention_blocks.attention.MultiHeadAttentionPooling
    
    query_map_input:
      kernel_init:
        _target_: flax.linen.initializers.he_normal
    
    dot_product_attention:
      _target_: flax.linen.MultiHeadDotProductAttention
      num_heads: 3
      kernel_init:
        _target_: flax.linen.initializers.he_normal
    
    layer_norm: 
      _target_: flax.linen.LayerNorm
      epsilon: 1e-6
      reduction_axes: [1]
      feature_axes: [-1]
    
    mlp_block:
      _target_: multi_modal_transformers.attention_blocks.attention.MLPBlock
      dense:
        _target_: flax.linen.Dense
        features: 768
        kernel_init:
          _target_: flax.linen.initializers.he_normal
        use_bias: true
        bias_init:
          _target_: flax.linen.initializers.normal
      
      activation:
        _partial_: true
        _target_: flax.linen.relu
      
      norm:
        _target_: flax.linen.Dropout
        rate: 0.1
      
      dense_out:
        _target_: flax.linen.Dense
        features: 768
        kernel_init:
          _target_: flax.linen.initializers.he_normal
        use_bias: true
        bias_init:
          _target_: flax.linen.initializers.normal

